{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "aqUzpo-LG35Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2add615c-5bca-417a-8f88-efbcd2c6b562"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path='drive/My Drive/Text Analysis'"
      ],
      "metadata": {
        "id": "wLNYo8izHav7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary pacakages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re"
      ],
      "metadata": {
        "id": "ZAkD7wLc_k5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718ac1db-4f90-4200-ecf7-c45cfd17b231"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the webpage containing the data\n",
        "url = 'https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/'\n",
        "\n",
        "# Send a GET request to fetch the webpage content\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Create a BeautifulSoup object\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the data you need from the webpage\n",
        "    # For example, find all <p> tags to get the text content\n",
        "    paragraphs = soup.find_all('p')\n",
        "\n",
        "    # Process the extracted data as needed\n",
        "    # For example, create a DataFrame from the extracted text\n",
        "    df = pd.DataFrame({'Text': [p.get_text() for p in paragraphs]})\n",
        "\n",
        "    # Print the DataFrame\n",
        "    print(df)\n",
        "else:\n",
        "    print(\"Failed to fetch the webpage:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "TxFuqnA6AC71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5bfc68-1ad0-4035-e757-6fe5a7ef33b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Text\n",
            "0   Efficient Supply Chain Assessment: Overcoming ...\n",
            "1   Streamlined Integration: Interactive Brokers A...\n",
            "2   Efficient Data Integration and User-Friendly I...\n",
            "3   Effective Management of Social Media Data Extr...\n",
            "4                               AI Bot Audio to audio\n",
            "5   Methodology for ETL Discovery Tool using LLMA,...\n",
            "6   Methodology for database discovery tool using ...\n",
            "7                             Chatbot using VoiceFlow\n",
            "8   Rising IT cities and its impact on the economy...\n",
            "9   Rising IT Cities and Their Impact on the Econo...\n",
            "10  Internet Demand’s Evolution, Communication Imp...\n",
            "11  Rise of Cybercrime and its Effect in upcoming ...\n",
            "12                      AI/ML and Predictive Modeling\n",
            "13               Solution for Contact Centre Problems\n",
            "14  How to Setup Custom Domain for Google App Engi...\n",
            "15                              Code Review Checklist\n",
            "16  We have seen a huge development and dependence...\n",
            "17                                   Rising IT cities\n",
            "18  Kolkata:- Kolkata in West Bengal is an emergin...\n",
            "19                                  Impact on Economy\n",
            "20  There is a huge impact of the rising IT cities...\n",
            "21                              Impact on Environment\n",
            "22  The rising IT cities will create a huge impact...\n",
            "23                           Impact on infrastructure\n",
            "24  There are many contributions of the IT cities ...\n",
            "25                                Impact on city life\n",
            "26  With the growth of IT cities, more people will...\n",
            "27  We provide intelligence, accelerate innovation...\n",
            "28                  Contact us: hello@blackcoffer.com\n",
            "29    © All Right Reserved, Blackcoffer(OPC) Pvt. Ltd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Directories\n",
        "text_file_path = \"/content/drive/MyDrive/Colab Notebooks/Text Analysis.docx\"\n",
        "stopwords_dir = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# Open the text file with a different encoding\n",
        "with open(text_file_path, 'r', encoding='ISO-8859-1') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize the text and remove stopwords\n",
        "stop_words = set()\n",
        "for filename in os.listdir(stopwords_dir):\n",
        "    with open(os.path.join(stopwords_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
        "        stop_words.update(set(f.read().splitlines()))\n",
        "\n",
        "words = word_tokenize(text)\n",
        "filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Add the filtered tokens to the docs list\n",
        "docs = [filtered_text]\n",
        "\n",
        "# store positive, Negative words from the directory\n",
        "pos = set()\n",
        "neg = set()\n",
        "\n",
        "# Assuming you have positive-words.txt and negative-words.txt in the same directory as the text file\n",
        "for filename in os.listdir(stopwords_dir):\n",
        "    if filename == 'positive-words.txt':\n",
        "        with open(os.path.join(stopwords_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
        "            pos.update(f.read().splitlines())\n",
        "    elif filename == 'negative-words.txt':\n",
        "        with open(os.path.join(stopwords_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
        "            neg.update(f.read().splitlines())\n",
        "\n",
        "# now collect the positive and negative words from each file\n",
        "# calculate the scores from the positive and negative words\n",
        "positive_words = []\n",
        "negative_words = []\n",
        "positive_score = []\n",
        "negative_score = []\n",
        "polarity_score = []\n",
        "subjectivity_score = []\n",
        "\n",
        "for doc in docs:\n",
        "    positive_words.append([word for word in doc if word.lower() in pos])\n",
        "    negative_words.append([word for word in doc if word.lower() in neg])\n",
        "    positive_score.append(len(positive_words[-1]))\n",
        "    negative_score.append(len(negative_words[-1]))\n",
        "    polarity_score.append((positive_score[-1] - negative_score[-1]) / ((positive_score[-1] + negative_score[-1]) + 0.000001))\n",
        "    subjectivity_score.append((positive_score[-1] + negative_score[-1]) / (len(doc) + 0.000001))\n"
      ],
      "metadata": {
        "id": "KXKpHjd1PSfE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure(file):\n",
        "    with open(os.path.join(text_dir, file), 'r', errors='ignore') as f:\n",
        "        text = f.read()\n",
        "        # rest of the function remains the same...\n"
      ],
      "metadata": {
        "id": "6bgCnuyqSqLh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import re\n",
        "import chardet\n",
        "\n",
        "avg_sentence_length = []\n",
        "Percentage_of_Complex_words = []\n",
        "Fog_Index = []\n",
        "complex_word_count = []\n",
        "avg_syllable_word_count = []\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def measure(file):\n",
        "    with open(os.path.join(text_dir, file), 'rb') as f:\n",
        "        raw_data = f.read()\n",
        "        encoding = chardet.detect(raw_data)['encoding']\n",
        "\n",
        "    with open(os.path.join(text_dir, file), 'r', encoding=encoding, errors='ignore') as f:\n",
        "        text = f.read()\n",
        "        # remove punctuations\n",
        "        text = re.sub(r'[^\\w\\s.]', '', text)\n",
        "        # split the given text file into sentences\n",
        "        sentences = text.split('.')\n",
        "        # total number of sentences in a file\n",
        "        num_sentences = len(sentences)\n",
        "        # total words in the file\n",
        "        words = [word for word in text.split() if word.lower() not in stopwords]\n",
        "        num_words = len(words)\n",
        "\n",
        "        # complex words having syllable count is greater than 2\n",
        "        # Complex words are words in the text that contain more than two syllables.\n",
        "        complex_words = [word for word in words if len(re.findall(r'[aeiou]+', word.lower())) > 2]\n",
        "\n",
        "        # Syllable Count Per Word\n",
        "        # We count the number of Syllables in each word of the text by counting the vowels present in each word.\n",
        "        # We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n",
        "        syllable_count = 0\n",
        "        syllable_words = []\n",
        "        for word in words:\n",
        "            if word.endswith('es'):\n",
        "                word = word[:-2]\n",
        "            elif word.endswith('ed'):\n",
        "                word = word[:-2]\n",
        "            vowels = 'aeiou'\n",
        "            syllable_count_word = sum(1 for letter in word if letter.lower() in vowels)\n",
        "            if syllable_count_word >= 1:\n",
        "                syllable_words.append(word)\n",
        "                syllable_count += syllable_count_word\n",
        "\n",
        "        avg_sentence_len = num_words / num_sentences\n",
        "        avg_syllable_word_count = syllable_count / len(syllable_words)\n",
        "        Percent_Complex_words = len(complex_words) / num_words\n",
        "        Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n",
        "\n",
        "        return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words), avg_syllable_word_count\n",
        "\n",
        "# Provide the directory path containing the text files\n",
        "text_dir = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# iterate through each file or doc\n",
        "for file in os.listdir(text_dir):\n",
        "    x, y, z, a, b = measure(file)\n",
        "    avg_sentence_length.append(x)\n",
        "    Percentage_of_Complex_words.append(y)\n",
        "    Fog_Index.append(z)\n",
        "    complex_word_count.append(a)\n",
        "    avg_syllable_word_count.append(b)\n"
      ],
      "metadata": {
        "id": "OtXbS1IEwb0M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Define the directory containing the text files\n",
        "text_dir = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# Load stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def cleaned_words(file):\n",
        "    with open(os.path.join(text_dir, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        text = f.read()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        words = [word for word in text.split() if word.lower() not in stopwords]\n",
        "        length = sum(len(word) for word in words)\n",
        "        average_word_length = length / len(words)\n",
        "    return len(words), average_word_length\n",
        "\n",
        "def count_personal_pronouns(file):\n",
        "    with open(os.path.join(text_dir, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        text = f.read()\n",
        "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
        "        count = 0\n",
        "        for pronoun in personal_pronouns:\n",
        "            count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text))\n",
        "    return count\n",
        "\n",
        "word_count = []\n",
        "average_word_length = []\n",
        "pp_count = []\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for file in os.listdir(text_dir):\n",
        "    # Calculate word count and average word length\n",
        "    words, avg_length = cleaned_words(file)\n",
        "    word_count.append(words)\n",
        "    average_word_length.append(avg_length)\n",
        "\n",
        "    # Calculate personal pronoun count\n",
        "    pp_count.append(count_personal_pronouns(file))\n"
      ],
      "metadata": {
        "id": "ELY_k2g7yQTB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30FwL3_FzLXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}